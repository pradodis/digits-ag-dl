# -*- coding: utf-8 -*-
"""OficialAG2

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1a2br2mXHaRpi-WStrJQEOahvRKVO7zyz
"""

import numpy as np
import tensorflow as tf
from tensorflow import keras
from sklearn.datasets import load_digits
from sklearn.model_selection import train_test_split
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns # Para gr√°ficos mais bonitos

# Configura√ß√µes para reprodutibilidade
np.random.seed(42)
tf.random.set_seed(42)

print("Setup inicial e importa√ß√µes conclu√≠dos.")

# --- Fun√ß√µes do Deep Learning ---
def load_and_preprocess_data():
    digits = load_digits()
    X = digits.data / 16.0
    y = digits.target
    x_treino, x_teste, y_treino, y_teste = train_test_split(X, y, test_size=0.2, random_state=42)
    return x_treino, x_teste, y_treino, y_teste, digits.images

def build_model_for_ag_fitness(input_shape, hidden_layer_1_weights=None, hidden_layer_1_biases=None):
    """
    Constr√≥i o modelo Keras.
    Se hidden_layer_1_weights/biases forem fornecidos, define-os e congela a camada.
    Caso contr√°rio, a camada ser√° trein√°vel.
    """
    model = keras.Sequential([
        keras.layers.InputLayer(shape=input_shape),
        keras.layers.Dense(128, activation='relu', name='ag_hidden_layer'), # Nomeamos para f√°cil acesso
        keras.layers.Dense(64, activation='relu'),
        keras.layers.Dense(10, activation='softmax')
    ])

    if hidden_layer_1_weights is not None and hidden_layer_1_biases is not None:
        # Define os pesos da camada que o AG est√° otimizando
        model.get_layer('ag_hidden_layer').set_weights([hidden_layer_1_weights, hidden_layer_1_biases])
        # Congela esta camada para que o otimizador Keras n√£o a altere
        model.get_layer('ag_hidden_layer').trainable = False

    # Compila o modelo. O otimizador s√≥ afetar√° as camadas trein√°veis.
    model.compile(
        loss='sparse_categorical_crossentropy',
        optimizer='rmsprop',
        metrics=['sparse_categorical_accuracy']
    )
    return model

# --- Fun√ß√µes Auxiliares para Manipula√ß√£o de Pesos ---
def flatten_weights(weights_list):
    """Achata uma lista de arrays de pesos (W, b) em um √∫nico vetor."""
    flattened = []
    for arr in weights_list:
        flattened.extend(arr.flatten())
    return np.array(flattened)

def unflatten_weights(flat_weights, layer_shapes):
    """
    Remodela um vetor achatado de pesos de volta para o formato original (W, b).
    `layer_shapes` deve ser uma lista de tuplas (shape_W, shape_b) para a camada.
    """
    reshaped_weights = []
    current_index = 0
    for w_shape, b_shape in layer_shapes:
        # Pesos (W)
        size_w = np.prod(w_shape)
        w = flat_weights[current_index : current_index + size_w].reshape(w_shape)
        reshaped_weights.append(w)
        current_index += size_w

        # Vieses (b)
        size_b = np.prod(b_shape)
        b = flat_weights[current_index : current_index + size_b].reshape(b_shape)
        reshaped_weights.append(b)
        current_index += size_b
    return reshaped_weights

print("Fun√ß√µes base e auxiliares definidas.")

# --- Carregar e Preparar Dados ---
x_treino, x_teste, y_treino, y_teste, _ = load_and_preprocess_data()

# Para a fun√ß√£o de aptid√£o, usaremos uma parte do treino para valida√ß√£o interna do AG
# E o conjunto de teste final para a avalia√ß√£o do modelo com os melhores pesos do AG.
x_ag_val, x_ag_train, y_ag_val, y_ag_train = train_test_split(x_treino, y_treino, test_size=0.7, random_state=42)

# Determinar as formas dos pesos da camada oculta que o AG vai otimizar
# (Entrada 64 -> Oculta 128)
# A camada 'ag_hidden_layer' tem √≠ndice 1 no modelo Sequential
temp_model = build_model_for_ag_fitness(input_shape=[64])
hidden_layer_1_weights_keras_format = temp_model.get_layer('ag_hidden_layer').get_weights()
hidden_layer_shapes = [(hidden_layer_1_weights_keras_format[0].shape, hidden_layer_1_weights_keras_format[1].shape)]
individual_size = len(flatten_weights(hidden_layer_1_weights_keras_format))

print(f"N√∫mero de neur√¥nios de entrada: {x_treino.shape[1]}")
print(f"Tamanho de um indiv√≠duo AG (vetor de pesos achatado): {individual_size}")

print("\nResumo da arquitetura da MLP para inspe√ß√£o:")
temp_model.summary()

# --- Par√¢metros do Algoritmo Gen√©tico ---
POPULATION_SIZE = 50       # N√∫mero de indiv√≠duos na popula√ß√£o
NUM_GENERATIONS = 30       # N√∫mero de gera√ß√µes a serem executadas
MUTATION_RATE = 0.05       # Probabilidade de um peso sofrer muta√ß√£o
CROSSOVER_PROBABILITY = 0.8 # Probabilidade de dois pais cruzarem
NUM_PARENTS_TO_SELECT = POPULATION_SIZE
ELITE_COUNT = 2            # Quantos dos melhores indiv√≠duos passam diretamente para a pr√≥xima gera√ß√£o (elitismo)

print("\nPar√¢metros do Algoritmo Gen√©tico definidos:")
params_df = pd.DataFrame({
    'Par√¢metro': ['POPULATION_SIZE', 'NUM_GENERATIONS', 'MUTATION_RATE', 'CROSSOVER_PROBABILITY', 'ELITE_COUNT', 'Individual Size'],
    'Valor': [POPULATION_SIZE, NUM_GENERATIONS, MUTATION_RATE, CROSSOVER_PROBABILITY, ELITE_COUNT, individual_size]
})
print(params_df.to_string(index=False))

# --- Fun√ß√µes do Algoritmo Gen√©tico ---

### **Passo 1: Gerar Popula√ß√£o Inicial**
def generate_initial_population(pop_size, individual_size):
    population = [np.random.uniform(low=-1.0, high=1.0, size=individual_size) for _ in range(pop_size)]
    return population

### **Passo 2: Avaliar Aptid√£o (Fitness)**
def evaluate_fitness(individual, x_val, y_val, model_input_shape, hidden_layer_shapes_info):
    weights_w_b = unflatten_weights(individual, hidden_layer_shapes_info)
    hidden_weights = weights_w_b[0]
    hidden_biases = weights_w_b[1]

    model = build_model_for_ag_fitness(model_input_shape,
                                       hidden_layer_1_weights=hidden_weights,
                                       hidden_layer_1_biases=hidden_biases)

    # Pequeno treino para as outras camadas se adaptarem
    model.fit(x_ag_train, y_ag_train, epochs=5, verbose=0)

    _, accuracy = model.evaluate(x_val, y_val, verbose=0)
    return accuracy

### **Passo 3: Verificar Condi√ß√£o de T√©rmino** (Fun√ß√£o de monitoramento)
def check_termination_condition(generation, max_generations, best_fitness_current_gen):
    return generation >= max_generations

### **Passo 4: Sele√ß√£o**
def select_parents(population, fitness_scores, num_parents_to_select, elite_count=1):
    sorted_indices = np.argsort(fitness_scores)[::-1]
    sorted_population = [population[i] for i in sorted_indices]

    parents = sorted_population[:elite_count]

    for _ in range(num_parents_to_select - elite_count):
        # Sele√ß√£o por Torneio: seleciona um pequeno grupo e escolhe o melhor
        candidate_indices = np.random.choice(len(population), size=min(5, len(population)), replace=False)
        tournament_candidates = [population[i] for i in candidate_indices]
        tournament_fitness = [fitness_scores[i] for i in candidate_indices]
        best_in_tournament_idx = np.argmax(tournament_fitness)
        parents.append(tournament_candidates[best_in_tournament_idx])

    return parents

### **Passo 5: Cruzamento (Crossover)**
def crossover(parent1, parent2):
    crossover_point = np.random.randint(1, len(parent1))
    child1 = np.concatenate((parent1[:crossover_point], parent2[crossover_point:]))
    child2 = np.concatenate((parent2[:crossover_point], parent1[crossover_point:]))
    return child1, child2

### **Passo 6: Muta√ß√£o**
def mutate(individual, mutation_rate, mutation_strength=0.1):
    for i in range(len(individual)):
        if np.random.rand() < mutation_rate:
            individual[i] += np.random.uniform(-mutation_strength, mutation_strength)
            individual[i] = np.clip(individual[i], -1.0, 1.0) # Garante limites de pesos
    return individual

### **Passo 7: Substituir Popula√ß√£o Antiga**
def replace_population(old_population, parents, new_children, pop_size, elite_count=1):
    new_population_local = parents[:elite_count]

    current_children_idx = 0
    while len(new_population_local) < pop_size and current_children_idx < len(new_children):
        new_population_local.append(new_children[current_children_idx])
        current_children_idx += 1

    while len(new_population_local) < pop_size: # Preenche se ainda faltar (raro)
        new_population_local.append(np.random.uniform(low=-1.0, high=1.0, size=individual_size))

    return new_population_local

print("Fun√ß√µes do Algoritmo Gen√©tico definidas.")

print("\n--- Iniciando Otimiza√ß√£o com Algoritmo Gen√©tico ---")
print(f"Par√¢metros: Popula√ß√£o={POPULATION_SIZE}, Gera√ß√µes={NUM_GENERATIONS}, Muta√ß√£o={MUTATION_RATE}, Crossover={CROSSOVER_PROBABILITY}")

# Vari√°veis para capturar o progresso
history_best_fitness = []
history_avg_fitness = []
best_fitness_overall = -np.inf
best_individual_overall = None
all_individuals_data = [] # Para an√°lise de diversidade

# 1. Gerar Popula√ß√£o Inicial
population = generate_initial_population(POPULATION_SIZE, individual_size)

print(f"\nPopula√ß√£o inicial gerada. Total de indiv√≠duos: {len(population)}")

# Loop para os passos 2 a 7 (Gera√ß√µes)
for generation in range(NUM_GENERATIONS):
    print(f"\n--- Gera√ß√£o {generation + 1}/{NUM_GENERATIONS} ---")

    # 2. Avaliar Aptid√£o
    print("  Avaliando Aptid√£o da Popula√ß√£o...")
    # CORRE√á√ÉO: x_treino.shape[1] envolto em uma tupla para o input_shape
    fitness_scores = [evaluate_fitness(ind, x_ag_val, y_ag_val, (x_treino.shape[1],), hidden_layer_shapes) for ind in population]

    current_best_fitness = np.max(fitness_scores)
    current_avg_fitness = np.mean(fitness_scores)
    current_best_individual_idx = np.argmax(fitness_scores)
    current_best_individual = population[current_best_individual_idx]

    history_best_fitness.append(current_best_fitness)
    history_avg_fitness.append(current_avg_fitness)

    # Captura dados de indiv√≠duos para an√°lise de diversidade (ex: para a primeira e √∫ltima gera√ß√£o)
    if generation == 0 or generation == NUM_GENERATIONS - 1:
        for idx, ind in enumerate(population):
            all_individuals_data.append({
                'Generation': generation + 1,
                'Individual_ID': idx,
                'Fitness': fitness_scores[idx],
                'Weights_Mean': np.mean(ind),
                'Weights_Std': np.std(ind)
            })

    if current_best_fitness > best_fitness_overall:
        best_fitness_overall = current_best_fitness
        best_individual_overall = current_best_individual

    print(f"  Melhor Aptid√£o na Gera√ß√£o: {current_best_fitness:.4f}")
    print(f"  M√©dia de Aptid√£o na Gera√ß√£o: {current_avg_fitness:.4f}")

    # 3. Verificar Condi√ß√£o de T√©rmino (monitoramento, o loop j√° √© a condi√ß√£o principal)
    # check_termination_condition(generation, NUM_GENERATIONS, current_best_fitness) # J√° monitorado pelos prints

    # 4. Sele√ß√£o
    # print("  Realizando Sele√ß√£o dos Pais...")
    parents = select_parents(population, fitness_scores, NUM_PARENTS_TO_SELECT, ELITE_COUNT)

    new_children = []
    num_children_needed = POPULATION_SIZE - ELITE_COUNT

    # 5. Cruzamento
    # print("  Realizando Cruzamento...")
    for i in range(0, NUM_PARENTS_TO_SELECT, 2):
        if i + 1 < NUM_PARENTS_TO_SELECT and np.random.rand() < CROSSOVER_PROBABILITY:
            child1, child2 = crossover(parents[i], parents[i+1])
            new_children.append(child1)
            new_children.append(child2)
        else:
            new_children.append(parents[i])
            if i + 1 < NUM_PARENTS_TO_SELECT:
                new_children.append(parents[i+1])

    new_children = new_children[:num_children_needed] # Ajusta para o n√∫mero exato necess√°rio

    # 6. Muta√ß√£o
    # print("  Aplicando Muta√ß√£o aos Filhos...")
    new_children = [mutate(child, MUTATION_RATE) for child in new_children]

    # 7. Substituir Popula√ß√£o Antiga
    # print("  Substituindo Popula√ß√£o...")
    population = replace_population(population, parents, new_children, POPULATION_SIZE, ELITE_COUNT)

print("\n--- Otimiza√ß√£o com AG Conclu√≠da! ---")
print(f"Melhor Aptid√£o Geral Encontrada: {best_fitness_overall:.4f}")

plt.figure(figsize=(12, 6))
plt.plot(range(1, NUM_GENERATIONS + 1), history_best_fitness, label='Melhor Aptid√£o', marker='o')
plt.plot(range(1, NUM_GENERATIONS + 1), history_avg_fitness, label='M√©dia de Aptid√£o', marker='x')
plt.title('Evolu√ß√£o da Aptid√£o do Algoritmo Gen√©tico por Gera√ß√£o')
plt.xlabel('Gera√ß√£o')
plt.ylabel('Acur√°cia (Aptid√£o)')
plt.grid(True)
plt.legend()
plt.xticks(range(1, NUM_GENERATIONS + 1, max(1, NUM_GENERATIONS // 10))) # Ajusta ticks para melhor visualiza√ß√£o
plt.show()

individuals_df = pd.DataFrame(all_individuals_data)

plt.figure(figsize=(14, 6))
sns.histplot(individuals_df[individuals_df['Generation'] == 1]['Weights_Mean'],
             color='blue', label='M√©dia de Pesos (Gera√ß√£o 1)', kde=True, stat='density', alpha=0.6)
sns.histplot(individuals_df[individuals_df['Generation'] == NUM_GENERATIONS]['Weights_Mean'],
             color='red', label=f'M√©dia de Pesos (Gera√ß√£o {NUM_GENERATIONS})', kde=True, stat='density', alpha=0.6)
plt.title('Distribui√ß√£o da M√©dia dos Pesos dos Indiv√≠duos por Gera√ß√£o')
plt.xlabel('M√©dia dos Valores dos Pesos')
plt.ylabel('Densidade')
plt.legend()
plt.grid(axis='y', alpha=0.75)
plt.show()

plt.figure(figsize=(14, 6))
sns.histplot(individuals_df[individuals_df['Generation'] == 1]['Weights_Std'],
             color='blue', label='Desvio Padr√£o dos Pesos (Gera√ß√£o 1)', kde=True, stat='density', alpha=0.6)
sns.histplot(individuals_df[individuals_df['Generation'] == NUM_GENERATIONS]['Weights_Std'],
             color='red', label=f'Desvio Padr√£o dos Pesos (Gera√ß√£o {NUM_GENERATIONS})', kde=True, stat='density', alpha=0.6)
plt.title('Distribui√ß√£o do Desvio Padr√£o dos Pesos dos Indiv√≠duos por Gera√ß√£o')
plt.xlabel('Desvio Padr√£o dos Valores dos Pesos')
plt.ylabel('Densidade')
plt.legend()
plt.grid(axis='y', alpha=0.75)
plt.show()

print("\nAplicando os melhores pesos encontrados pelo AG ao modelo final...")

# Remodelar o melhor indiv√≠duo para o formato Keras
best_weights_w_b = unflatten_weights(best_individual_overall, hidden_layer_shapes)
final_hidden_weights = best_weights_w_b[0]
final_hidden_biases = best_weights_w_b[1]

# Constr√≥i o modelo final com os pesos AG e treina as outras camadas
final_model_ag_optimized = build_model_for_ag_fitness(
    input_shape=[64],
    hidden_layer_1_weights=final_hidden_weights,
    hidden_layer_1_biases=final_hidden_biases
)

print("\nTreinando as outras camadas do modelo final para se adaptarem aos pesos AG...")
# Treine as camadas "n√£o AG" no conjunto de treinamento completo
final_model_ag_optimized.fit(x_treino, y_treino, epochs=50, verbose=1)

print("\nAvaliando o Modelo Final Otimizado com AG no Conjunto de Teste Completo:")
loss_ag_final, accuracy_ag_final = final_model_ag_optimized.evaluate(x_teste, y_teste, verbose=0)
print(f"Perda no Teste (Modelo AG-Otimizado): {loss_ag_final:.4f}")
print(f"Acur√°cia no Teste (Modelo AG-Otimizado): {accuracy_ag_final:.4f}")

# Exemplo de relat√≥rio final em tabela
final_results_df = pd.DataFrame({
    'M√©trica': ['Melhor Aptid√£o do AG', 'Acur√°cia Final no Teste', 'Perda Final no Teste'],
    'Valor': [f'{best_fitness_overall:.4f}', f'{accuracy_ag_final:.4f}', f'{loss_ag_final:.4f}']
})
print("\n--- Resumo dos Resultados Finais ---")
print(final_results_df.to_string(index=False))

"""# =============================
# 7. Conclus√µes e Pr√≥ximos Passos
# =============================

 Este relat√≥rio apresenta um fluxo completo de otimiza√ß√£o h√≠brida combinando Algoritmos Gen√©ticos (AG) e Deep Learning.
As visualiza√ß√µes ajudam a compreender a evolu√ß√£o da popula√ß√£o e o impacto da otimiza√ß√£o gen√©tica nos pesos da rede neural.

üîç Destaques:
- A curva de aptid√£o (melhor e m√©dia) por gera√ß√£o evidencia que o AG √© capaz de encontrar solu√ß√µes progressivamente melhores.
 - A an√°lise de distribui√ß√£o dos pesos mostra como o AG direciona a configura√ß√£o da camada otimizada ao longo das gera√ß√µes.
- A acur√°cia final no conjunto de teste valida a efic√°cia do modelo h√≠brido.

üöÄ Pr√≥ximos Passos Sugeridos:
1. Compara√ß√£o com modelo cl√°ssico: treinar uma MLP puramente com RMSprop (sem AG) para comparar os resultados.
2. Ajuste de hiperpar√¢metros do AG: testar diferentes tamanhos de popula√ß√£o, taxas de muta√ß√£o, probabilidades de cruzamento e n√∫mero de gera√ß√µes.
3. Otimiza√ß√£o de m√∫ltiplas camadas: aplicar AG na configura√ß√£o de mais de uma camada ou at√© na arquitetura completa da rede.
4. Visualiza√ß√£o dos indiv√≠duos: explorar como os "melhores" pesos influenciam a representa√ß√£o interna dos dados.
"""

